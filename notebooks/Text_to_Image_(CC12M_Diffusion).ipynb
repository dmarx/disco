{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text to Image (CC12M Diffusion).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generates images from text prompts.\n",
        "\n",
        "By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses a 602M parameter diffusion model trained on Conceptual 12M. See the GitHub repo for more information: https://github.com/crowsonkb/v-diffusion-pytorch."
      ],
      "metadata": {
        "id": "sThFl4fJtEQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2022 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "metadata": {
        "cellView": "form",
        "id": "eb9FKu1rtH6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa4Er6nMgI1b"
      },
      "outputs": [],
      "source": [
        "# Check the GPU\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies (no need to rerun this section if you restart the notebook runtime)"
      ],
      "metadata": {
        "id": "0XknUUiEsPCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "\n",
        "!pip install ftfy regex requests tqdm\n",
        "!git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch"
      ],
      "metadata": {
        "id": "xQVAY5-4g8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the diffusion model\n",
        "# SHA-256: 4fc95ee1b3205a3f7422a07746383776e1dbc367eaf06a5b658ad351e77b7bda\n",
        "\n",
        "!mkdir v-diffusion-pytorch/checkpoints\n",
        "!curl -L https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth > v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth"
      ],
      "metadata": {
        "id": "9410kZ2aipVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import modules and load models"
      ],
      "metadata": {
        "id": "tbjtOS68sazS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import gc\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "import torch\n",
        "from torchvision import utils as tv_utils\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "sys.path.append('/content/v-diffusion-pytorch')\n",
        "\n",
        "from CLIP import clip\n",
        "from diffusion import get_model, sampling, utils"
      ],
      "metadata": {
        "id": "f_6OhF85i5vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the models\n",
        "\n",
        "model = get_model('cc12m_1_cfg')()\n",
        "_, side_y, side_x = model.shape\n",
        "model.load_state_dict(torch.load('v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth', map_location='cpu'))\n",
        "model = model.half().cuda().eval().requires_grad_(False)\n",
        "clip_model = clip.load(model.clip_model, jit=False, device='cpu')[0]"
      ],
      "metadata": {
        "id": "v7Zl0mg1jrBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "\n",
        "#@markdown The text prompt\n",
        "prompt = 'New York City, oil on canvas'  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The strength of the text conditioning (0 means don't condition on text, 1 means sample images that match the text about as well as the images match the text captions in the training set, 3+ is recommended).\n",
        "weight = 5  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Sample this many images.\n",
        "n_images = 4  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Specify the number of diffusion timesteps (default is 50, can lower for faster but lower quality sampling).\n",
        "steps = 50  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown The random seed. Change this to sample different images.\n",
        "seed = 0  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Display progress every this many timesteps.\n",
        "display_every =   10#@param {type:\"integer\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kitsmKsekOJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually do the run..."
      ],
      "metadata": {
        "id": "kuTFyUdbrlcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_embed = clip_model.encode_text(clip.tokenize(prompt)).float().cuda()\n",
        "\n",
        "\n",
        "def cfg_model_fn(x, t):\n",
        "    \"\"\"The CFG wrapper function.\"\"\"\n",
        "    n = x.shape[0]\n",
        "    x_in = x.repeat([2, 1, 1, 1])\n",
        "    t_in = t.repeat([2])\n",
        "    clip_embed_repeat = target_embed.repeat([n, 1])\n",
        "    clip_embed_in = torch.cat([torch.zeros_like(clip_embed_repeat), clip_embed_repeat])\n",
        "    v_uncond, v_cond = model(x_in, t_in, clip_embed_in).chunk(2, dim=0)\n",
        "    v = v_uncond + (v_cond - v_uncond) * weight\n",
        "    return v\n",
        "\n",
        "\n",
        "def display_callback(info):\n",
        "    if info['i'] % display_every == 0:\n",
        "        nrow = math.ceil(info['pred'].shape[0]**0.5)\n",
        "        grid = tv_utils.make_grid(info['pred'], nrow, padding=0)\n",
        "        tqdm.write(f'Step {info[\"i\"]} of {steps}:')\n",
        "        display.display(utils.to_pil_image(grid))\n",
        "        tqdm.write(f'')\n",
        "\n",
        "\n",
        "def run():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.manual_seed(seed)\n",
        "    x = torch.randn([n_images, 3, side_y, side_x], device='cuda')\n",
        "    t = torch.linspace(1, 0, steps + 1, device='cuda')[:-1]\n",
        "    step_list = utils.get_spliced_ddpm_cosine_schedule(t)\n",
        "    outs = sampling.plms_sample(cfg_model_fn, x, step_list, {}, callback=display_callback)\n",
        "    tqdm.write('Done!')\n",
        "    for i, out in enumerate(outs):\n",
        "        filename = f'out_{i}.png'\n",
        "        utils.to_pil_image(out).save(filename)\n",
        "        display.display(display.Image(filename))\n",
        "\n",
        "\n",
        "run()"
      ],
      "metadata": {
        "id": "MnjPM_PUkYui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}