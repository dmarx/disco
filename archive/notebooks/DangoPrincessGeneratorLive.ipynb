{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DangoPrincessGeneratorLive",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "9XFGZqR1O7lh",
        "13oKw6GdAHzM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Princess Generator: Ver. Victoria\n",
        "\n",
        "Edited to run on Google Colab by HostsServer (https://github.com/MSFTserver)\n",
        "\n",
        "Originally by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses a 512x512 unconditional ImageNet diffusion model fine-tuned from OpenAI's 512x512 class-conditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images.\n",
        "\n",
        "Now updated using V-diffusion by Katherine Crowson. (https://github.com/crowsonkb/v-diffusion-pytorch)\n",
        "\n",
        "@Nshepperd, @DaneilRussruss also have contribution in this code. I may missed some of the contributors of specific functions. If you found your credit is missing, let me know and I 'll add it as soon as possible."
      ],
      "metadata": {
        "id": "P4EOBDjsjUc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ 1 ] **Get Things Ready**\n",
        "section **[ 1.4 ]** has mounting drive option (unfold this cell to see it)"
      ],
      "metadata": {
        "id": "9XFGZqR1O7lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.1 ] **Check GPU Status**\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "6dKLZXv7XJ0e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.2 ] **download needed dependencies**\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/guided-diffusion\n",
        "!git clone https://github.com/crowsonkb/v-diffusion-pytorch\n",
        "!git clone https://github.com/assafshocher/ResizeRight.git\n",
        "!pip install piq lpips ftfy regex tqdm pytorch_lit\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./guided-diffusion"
      ],
      "metadata": {
        "id": "UV0RLATky34T",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.3 ] **Import Libraries ðŸ“š**\n",
        "import os, gc, io, re, math, time, sys, random, lpips, shutil, requests, torch, threading\n",
        "sys.path.append('./ResizeRight')\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./v-diffusion-pytorch')\n",
        "sys.path.append('./guided-diffusion')\n",
        "import clip\n",
        "from google.colab import drive\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from os import path\n",
        "import numpy as np\n",
        "from piq import brisque\n",
        "from itertools import product\n",
        "from IPython import display\n",
        "from PIL import Image, ImageOps\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "from numpy import nan\n",
        "from resize_right import resize, calc_pad_sz\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from tqdm import trange\n",
        "from diffusion import get_model, get_models, utils\n",
        "from pytorch_lit import LitModule\n",
        "from urllib.parse import urlparse\n",
        "import torch.hub as hub"
      ],
      "metadata": {
        "id": "LZnkxLpvzY2m",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.4 ] **Prepare Folders**\n",
        "#@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
        "\n",
        "google_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Click here if you'd like to save the model checkpoint file to (and/or load from) your Google Drive:\n",
        "yes_please = False #@param {type:\"boolean\"}\n",
        "\n",
        "if google_drive is True:\n",
        "  drive.mount('/content/drive')\n",
        "  root_path = '/content/drive/MyDrive/Hosts-AI-Edits/DangoPrincessLive'\n",
        "else:\n",
        "  root_path = '/content'\n",
        "\n",
        "#Simple create paths taken with modifications from Datamosh's Batch VQGAN+CLIP notebook\n",
        "def createPath(filepath):\n",
        "    if path.exists(filepath) == False:\n",
        "      os.makedirs(filepath)\n",
        "      print(f'Made {filepath}')\n",
        "    else:\n",
        "      print(f'filepath {filepath} exists.')\n",
        "\n",
        "outDirPath = f'{root_path}/images_out'\n",
        "createPath(outDirPath)\n",
        "createPath(f'{root_path}/diff')\n",
        "\n",
        "if google_drive and not yes_please or not google_drive:\n",
        "    model_path = '/content/models'\n",
        "    lpips_model_path = f'{model_path}/lpips/vgg'\n",
        "    clip_model_path = f'{model_path}/CLIP'\n",
        "if google_drive and yes_please:\n",
        "    model_path = f'{root_path}/models'\n",
        "    lpips_model_path = f'{model_path}/lpips/vgg'\n",
        "    clip_model_path = f'{model_path}/CLIP'\n",
        "createPath(model_path)\n",
        "createPath(lpips_model_path)\n",
        "createPath(clip_model_path)"
      ],
      "metadata": {
        "id": "XmjK2E8b-6gL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.5 ] **Define necessary functions**\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "        \n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://') or prompt.startswith(\"E:\") or prompt.startswith(\"C:\") or prompt.startswith(\"D:\"):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutoutsVDango(nn.Module):\n",
        "    def __init__(self, cut_size,\n",
        "                 Overview=4, \n",
        "                 WholeCrop = 0, WC_Allowance = 10, WC_Grey_P=0.2,\n",
        "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.Overview = Overview\n",
        "        self.WholeCrop= WholeCrop\n",
        "        self.WC_Allowance = WC_Allowance\n",
        "        self.WC_Grey_P = WC_Grey_P\n",
        "        self.InnerCrop = InnerCrop\n",
        "        self.IC_Size_Pow = IC_Size_Pow\n",
        "        self.IC_Grey_P = IC_Grey_P\n",
        "        self.augs = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=5, \n",
        "                           translate=(0.05, 0.05), \n",
        "                           #scale=(0.9,0.95),\n",
        "                           fill=-1,  interpolation = T.InterpolationMode.BILINEAR, ),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            #T.RandomPerspective(p=1, interpolation = T.InterpolationMode.BILINEAR, fill=-1,distortion_scale=0.2),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomGrayscale(p=0.1),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        gray = transforms.Grayscale(3)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        l_size = max(sideX, sideY)\n",
        "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
        "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
        "        pad_input = F.pad(input,((sideY-max_size)//2+round(max_size*0.05),(sideY-max_size)//2+round(max_size*0.05),(sideX-max_size)//2+round(max_size*0.05),(sideX-max_size)//2+round(max_size*0.05)), **padargs)\n",
        "        cutouts_list = []\n",
        "        \n",
        "        if self.Overview>0:\n",
        "            cutouts = []\n",
        "            cutout = resize(pad_input, out_shape=output_shape)\n",
        "            if self.Overview in [1,2,4]:\n",
        "                if self.Overview>=2:\n",
        "                    cutout=torch.cat((cutout,gray(cutout)))\n",
        "                if self.Overview==4:\n",
        "                    cutout = torch.cat((cutout, TF.hflip(cutout)))\n",
        "            else:\n",
        "                output_shape_all = list(output_shape)\n",
        "                output_shape_all[0]=self.Overview\n",
        "                cutout = resize(pad_input, out_shape=output_shape_all)\n",
        "                if aug: cutout=self.augs(cutout)\n",
        "            cutouts_list.append(cutout)\n",
        "            \n",
        "        if self.InnerCrop >0:\n",
        "            cutouts=[]\n",
        "            for i in range(self.InnerCrop):\n",
        "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "                if i <= int(self.IC_Grey_P * self.InnerCrop):\n",
        "                    cutout = gray(cutout)\n",
        "                cutout = resize(cutout, out_shape=output_shape)\n",
        "                cutouts.append(cutout)\n",
        "            if cutout_debug:\n",
        "                TF.to_pil_image(cutouts[-1].add(1).div(2).clamp(0, 1).squeeze(0)).save(f\"{root_path}/diff/cutouts/cutout_InnerCrop.jpg\",quality=99)\n",
        "            cutouts_tensor = torch.cat(cutouts)\n",
        "            cutouts=[]\n",
        "            cutouts_list.append(cutouts_tensor)\n",
        "        cutouts=torch.cat(cutouts_list)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input, range_min, range_max):\n",
        "    return (input - input.clamp(range_min,range_max)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "def cond_clamp(image): \n",
        "    #if t >=0:\n",
        "        mag=image.square().mean().sqrt()\n",
        "        mag = (mag*cc).clamp(1.6,100)\n",
        "        image = image.clamp(-mag, mag)\n",
        "        return(image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cond_sample(model, x, steps, eta, extra_args, cond_fn, number):\n",
        "    \"\"\"Draws guided samples from a model given starting noise.\"\"\"\n",
        "    global clamp_max\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    alphas, sigmas = utils.t_to_alpha_sigma(steps)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(len(steps)):\n",
        "        if pace[i%len(pace)][\"model_name\"]==\"cc12m_1\":\n",
        "            extra_args_in = extra_args\n",
        "        else:\n",
        "            extra_args_in= {}\n",
        "\n",
        "        # Get the model output\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                if lerp:\n",
        "                    v=torch.zeros_like(x)\n",
        "                    for j in pace:\n",
        "                        if j[\"model_name\"]==\"cc12m_1\":\n",
        "                            extra_args_in = extra_args\n",
        "                        else:\n",
        "                            extra_args_in= {}\n",
        "                        v += model[j[\"model_name\"]](x, ts * steps[i], **extra_args_in)\n",
        "                    v = v/len(pace)\n",
        "                else:\n",
        "                    v = model[pace[i%len(pace)][\"model_name\"]](x, ts * steps[i], **extra_args_in)\n",
        "            v = cond_clamp(v)\n",
        "\n",
        "        if use_secondary_model:\n",
        "            with torch.no_grad():\n",
        "                if steps[i] < 1 and pace[i%len(pace)][\"guided\"]:\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    cond_grad = cond_fn(x, ts * steps[i],pred, **extra_args).detach()\n",
        "                    v = v.detach() - cond_grad * (sigmas[i] / alphas[i]) * pace[i%len(pace)][\"mag_adjust\"]\n",
        "                else:\n",
        "                    v = v.detach()\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    clamp_max=torch.tensor([0])\n",
        "\n",
        "        else:\n",
        "            if steps[i] < 1 and pace[i%len(pace)][\"guided\"]:\n",
        "                with torch.enable_grad():\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    cond_grad = cond_fn(x, ts * steps[i],pred, **extra_args).detach()\n",
        "                    v = v.detach() - cond_grad * (sigmas[i] / alphas[i]) * pace[i%len(pace)][\"mag_adjust\"]\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    v = v.detach()\n",
        "                    pred = x * alphas[i] - v * sigmas[i]\n",
        "                    clamp_max=torch.tensor([0])\n",
        "\n",
        "        mag = pred.square().mean().sqrt()\n",
        "      #  print(mag)\n",
        "        if torch.isnan(mag):\n",
        "            print(\"ERROR2\")\n",
        "            continue\n",
        "        \n",
        "        filename = f'{root_path}/diff/{taskname}_N.jpg'\n",
        "        number += 1\n",
        "        TF.to_pil_image(pred[0].add(1).div(2).clamp(0, 1)).save(filename,quality=99)\n",
        "        if save_iterations and number % save_every == 0:\n",
        "          shutil.copy(filename, f'{outDirPath}/{taskname}_{number}.jpg')\n",
        "        textprogress.value = f'{taskname},  step {round(steps[i].item()*1000)}, {pace[i%len(pace)][\"model_name\"]} :'\n",
        "        file = open(filename, \"rb\")\n",
        "        image=file.read()\n",
        "        progress.value = image \n",
        "        file.close()\n",
        "            \n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < len(steps) - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            if eta >=0:\n",
        "                ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                    (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            else:\n",
        "                ddim_sigma = -eta*sigmas[i+1]\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "            x = cond_clamp(x)\n",
        "\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "            \n",
        "         #######   x = sample_a_step(model, x.detach(), steps2, i//2, eta, extra_args)\n",
        "\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clamp_start_=0\n",
        "def cond_fn(x, t, x_in, clip_embed=[]):\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        global test, clamp_start_, clamp_max\n",
        "        t2=t\n",
        "        t=round(t.item()*1000)\n",
        "        n = x.shape[0]\n",
        "        with torch.enable_grad():\n",
        "            if use_secondary_model:                 \n",
        "                x = x.detach().requires_grad_()\n",
        "                x_in_second = secondary_model(x, t2.repeat([n])).pred\n",
        "                if use_original_as_clip_in: x_in = replace_grad(x_in, (1-use_original_as_clip_in)*x_in_second+use_original_as_clip_in*x_in)\n",
        "                else : x_in = x_in_second\n",
        "\n",
        "\n",
        "            x_in_grad = torch.zeros_like(x_in)\n",
        "            clip_guidance_scale = clip_guidance_index[1000-t]\n",
        "#             clamp_max = clamp_index[1000-t]\n",
        "            make_cutouts = {}\n",
        "            cutn = cut_innercut[1000-t] + cut_overview[1000-t]\n",
        "            for i in clip_list:\n",
        "                make_cutouts[i] = MakeCutoutsVDango(clip_size[i],\n",
        "                 Overview= cut_overview[1000-t], \n",
        "                 InnerCrop = cut_innercut[1000-t], IC_Size_Pow=cut_ic_pow, IC_Grey_P = cut_icgray_p[1000-t]\n",
        "                 )\n",
        "            nscut = MakeCutoutsVDango(200, Overview=1)\n",
        "            add_cuts = nscut(x_in.add(1).div(2))\n",
        "            for k in range(cutn_batches):\n",
        "                    losses=0\n",
        "                    for i in clip_list:\n",
        "                        clip_in = normalize(make_cutouts[i](x_in.add(1).div(2)).to(\"cuda\"))\n",
        "                        image_embeds = clip_model[i].encode_image(clip_in).float()\n",
        "                        image_embeds = image_embeds.unsqueeze(1)\n",
        "                        dists = spherical_dist_loss(image_embeds, target_embeds[i].unsqueeze(0))\n",
        "                        del image_embeds, clip_in\n",
        "                        dists = dists.view([cutn, n, -1])\n",
        "                        losses = dists.mul(weights[i]).sum(2).mean(0)\n",
        "                        x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches / len(clip_list)          \n",
        "                        del dists,losses\n",
        "                    gc.collect()\n",
        "                    \n",
        "            tv_losses = tv_loss(x_in).sum() * tv_scales[0] +\\\n",
        "                tv_loss(F.interpolate(x_in, scale_factor= 1/2)).sum()* tv_scales[1] + \\\n",
        "                tv_loss(F.interpolate(x_in, scale_factor = 1/4)).sum()* tv_scales[2] + \\\n",
        "                tv_loss(F.interpolate(x_in, scale_factor = 1/8)).sum()* tv_scales[3] \n",
        "            sat_scale = sat_index[1000-t]\n",
        "            range_scale= range_index[1000-t]\n",
        "            range_losses = range_loss(x_in,RGB_min,RGB_max).sum() * range_scale\n",
        "            sat_losses = range_loss(x,-1.0,1.0).sum() * sat_scale + tv_loss(x).sum() * tv_scale_2\n",
        "            try:\n",
        "                bsq_loss = brisque(x_in.add(1).div(2).clamp(0,1),data_range=1.)\n",
        "            except:\n",
        "                bsq_loss=0\n",
        "            if bsq_loss <=10 : bsq_loss = 0\n",
        "            \n",
        "            loss =  tv_losses  + range_losses  + \\\n",
        "                bsq_loss * bsq_scale \n",
        "\n",
        "            if init is not None and init_scale:\n",
        "                init_losses = lpips_model(x_in, init)\n",
        "                loss = loss + init_losses.sum() * init_scale\n",
        "            loss_grad = torch.autograd.grad(loss, x_in, )[0]\n",
        "            sat_grad = torch.autograd.grad(sat_losses, x, )[0]\n",
        "            x_in_grad += loss_grad + sat_grad\n",
        "            x_in_grad = torch.nan_to_num(x_in_grad, nan=0.0, posinf=0, neginf=0)\n",
        "            grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
        "            grad = torch.nan_to_num(grad, nan=0.0, posinf=0, neginf=0)\n",
        "            mag = grad.square().mean().sqrt()\n",
        "            if mag==0:\n",
        "                print(\"ERROR\")\n",
        "                return(grad)\n",
        "            if t>=0:\n",
        "                if active_function == \"softsign\":\n",
        "                    grad = F.softsign(grad*grad_scale/mag)\n",
        "                if active_function == \"tanh\":\n",
        "                    grad = (grad/mag*grad_scale).tanh()\n",
        "                if active_function==\"clamp\":\n",
        "                    grad = grad.clamp(-mag*grad_scale*2,mag*grad_scale*2)\n",
        "            if grad.abs().max()>0:\n",
        "                grad=grad/grad.abs().max()\n",
        "                magnitude = grad.square().mean().sqrt()\n",
        "            else:\n",
        "                print(grad)\n",
        "                return(grad)\n",
        "            clamp_max = clamp_index[1000-t]\n",
        "        return grad* magnitude.clamp(max= clamp_max) /magnitude\n",
        "\n",
        "\n",
        "\n",
        "def download_model(url, dst_path):\n",
        "    parts = urlparse(url)\n",
        "    filename = os.path.basename(parts.path)\n",
        "    n_filename = filename.split('-')[0]+'.pth'\n",
        "    if os.path.exists(f'{dst_path}/{n_filename}') is not True:\n",
        "      hub.download_url_to_file(url, os.path.join(dst_path, n_filename), None, True)\n",
        "    return filename"
      ],
      "metadata": {
        "id": "onrnIvUUzd29",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.6 ] **Define the secondary diffusion model**\n",
        "\n",
        "def append_dims(x, n):\n",
        "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
        "\n",
        "\n",
        "def expand_to_planes(x, shape):\n",
        "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
        "\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
        "\n",
        "\n",
        "def t_to_alpha_sigma(t):\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiffusionOutput:\n",
        "    v: torch.Tensor\n",
        "    pred: torch.Tensor\n",
        "    eps: torch.Tensor\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "class SecondaryDiffusionImageNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.down = nn.AvgPool2d(2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, cs[0]),\n",
        "            ConvBlock(cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                self.down,\n",
        "                ConvBlock(cs[0], cs[1]),\n",
        "                ConvBlock(cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    self.down,\n",
        "                    ConvBlock(cs[1], cs[2]),\n",
        "                    ConvBlock(cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        self.down,\n",
        "                        ConvBlock(cs[2], cs[3]),\n",
        "                        ConvBlock(cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            self.down,\n",
        "                            ConvBlock(cs[3], cs[4]),\n",
        "                            ConvBlock(cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                self.down,\n",
        "                                ConvBlock(cs[4], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[4]),\n",
        "                                self.up,\n",
        "                            ]),\n",
        "                            ConvBlock(cs[4] * 2, cs[4]),\n",
        "                            ConvBlock(cs[4], cs[3]),\n",
        "                            self.up,\n",
        "                        ]),\n",
        "                        ConvBlock(cs[3] * 2, cs[3]),\n",
        "                        ConvBlock(cs[3], cs[2]),\n",
        "                        self.up,\n",
        "                    ]),\n",
        "                    ConvBlock(cs[2] * 2, cs[2]),\n",
        "                    ConvBlock(cs[2], cs[1]),\n",
        "                    self.up,\n",
        "                ]),\n",
        "                ConvBlock(cs[1] * 2, cs[1]),\n",
        "                ConvBlock(cs[1], cs[0]),\n",
        "                self.up,\n",
        "            ]),\n",
        "            ConvBlock(cs[0] * 2, cs[0]),\n",
        "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
        "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
        "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "def wrapped_openai(x, t):\n",
        "    x = x\n",
        "    t = t\n",
        "    return openai(x, t * 1000)[:, :3]\n",
        "\n",
        "def cfg_model_fn(x, t):\n",
        "    n = x.shape[0]\n",
        "    n_conds = len(target_embeds[\"ViT-B/16\"])\n",
        "    x_in = x.repeat([n_conds, 1, 1, 1])\n",
        "    t_in = t.repeat([n_conds])\n",
        "    clip_embed_in = target_embeds[\"ViT-B/16\"].repeat_interleave(n, 0)\n",
        "    vs = model[\"cc12m_1_cfg\"](x_in, t_in, clip_embed_in).view([n_conds, n, *x.shape[1:]])\n",
        "    v = vs.mul(weights[\"ViT-B/16\"][:, None, None, None, None]).sum(0)\n",
        "    #display.clear_output(wait=True)\n",
        "    return v"
      ],
      "metadata": {
        "id": "VY3VihegziZ4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 1.7 ] Download Models!\n",
        "\n",
        "!wget -nc 'https://github.com/nshepperd/jax-guided-diffusion/raw/master/data/openimages_512x_png_embed224.npz' -P{model_path}\n",
        "!wget -nc 'https://github.com/nshepperd/jax-guided-diffusion/raw/master/data/imagenet_512x_jpg_embed224.npz' -P{model_path}\n",
        "\n",
        "# Download lpips Models\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
        "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
        "}\n",
        "\n",
        "for url in model_urls.values():\n",
        "    download_model(url, lpips_model_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WGfHnR6XegTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator Options**"
      ],
      "metadata": {
        "id": "QIv4JMHcPc-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 2.1 ] **Basic Options**\n",
        "title = \"AA\" #@param {type:'string'}\n",
        "prompt_list = [\"Portrait of Princess victoria, trending on artstation\"] #@param {type:'raw'}\n",
        "#@markdown\n",
        "save_iterations = True #@param{type:'boolean'}\n",
        "save_every = 1 #@param{type:\"raw\"}\n",
        "#@markdown\n",
        "batch_num=0 #@param {type:'raw'}\n",
        "clamp_start_=0 #@param {type:'raw'}\n",
        "seed = 42 #@param {type:'raw'}\n",
        "init_scale = 0#@param {type:'raw'} # This enhances the effect of the init image, a good value is 1000.\n",
        "init_image = None#@param {type:'raw'}\n",
        "use_lpips = False #@param {type:'boolean'}\n",
        "image_prompts = [] #@param {type:'raw'}\n",
        "mask_scale=0 #@param {type:'raw'}\n",
        "init_mask = None #@param {type:'raw'}\n",
        "#@markdown \n",
        "step = 250#@param {type:'raw'}\n",
        "steps_pow=1#@param {type:'raw'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bi7kVaXm-k4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Options - Must Run Even If Nothing Changed!\n",
        "play with them at your discretion or just run it to keep the defaults"
      ],
      "metadata": {
        "id": "13oKw6GdAHzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 2.2 ] **Advanced Options**\n",
        "#@markdown \n",
        "\n",
        "RGB_min, RGB_max = [-0.95,0.95] #@param {type:'raw'}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "n_batches = 1 #@param {type:'raw'}\n",
        "cutn_batches = 1#@param {type:'raw'}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "cut_overview = [10]*200+[10]*1000#@param {type:'raw'}\n",
        "cut_innercut = [0]*200+[0]* 1000#@param {type:'raw'}\n",
        "cut_ic_pow = 0.3#@param {type:'raw'}\n",
        "cut_icgray_p = [0]*100+[0]*100+[0]*100+[0]*1000#@param {type:'raw'}\n",
        "aug=True#@param {type:'boolean'}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "padargs = {\"mode\":\"constant\", \"value\":-1}#@param {type:'raw'}\n",
        "flip_aug=False#@param {type:'boolean'}\n",
        "cutout_debug = False#@param {type:'boolean'}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "clip_guidance_index = [240000]*1000#@param {type:'raw'}\n",
        "anti_jpg=0#@param {type:'raw'}\n",
        "w,h = 512,640#@param {type:'raw'}\n",
        "\n",
        "#@markdown \n",
        "#@markdown \n",
        "\n",
        "tv_scales = [0]+[2000]*3#@param {type:'raw'}\n",
        "tv_scale_2 = 0#@param {type:'raw'}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "clamp_index = [0.03]*50+[0.04]*100+[0.05]*850 #@param {type:'raw'}\n",
        "clamp_index = 1*np.array(clamp_index)\n",
        "sat_index =   [0]*40+[0]*960 #@param {type:'raw'}\n",
        "sat_index = np.array(sat_index)\n",
        "range_index = [0]*50 +[0]*950 #@param {type:'raw'}\n",
        "range_index = np.array(range_index)\n",
        "eta=1.8 #@param {type:'raw'} "
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZcLfZu33RAbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Models**"
      ],
      "metadata": {
        "id": "QYKLG9Fy91O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 3 ] **Load Diffusion models**\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "diff_file_name = '512x512_diffusion_uncond_openimages_epoch28_withfilter'\n",
        "yfcc_2_file_name = 'yfcc_2'\n",
        "cc12m_file_name = 'cc12m_1_cfg'\n",
        "secondary_file_name = 'secondary_model_imagenet_2'\n",
        "\n",
        "pace = []\n",
        "model_list = []\n",
        "diff_512_openimages = True #@param{type:'boolean'}\n",
        "yfcc_2 = True #@param{type:'boolean'}\n",
        "cc12m = False #@param{type:'boolean'}\n",
        "use_secondary_model=True #@param {type:'boolean'}\n",
        "#@markdown \n",
        "diffusion_steps = 1000#@param {type:'raw'}\n",
        "rescale_timesteps = True #@param {type:'boolean'}\n",
        "timestep_respacing = \"16,48,72\"#@param {type:'string'}\n",
        "use_original_as_clip_in=0 #@param {type:'raw'}\n",
        "lerp=False #@param {type:'boolean'}\n",
        "\n",
        "if diff_512_openimages:\n",
        "  if '{\"model_name\":\"512x512_diffusion_uncond_openimages_epoch28_withfilter\", \"guided\":True, \"mag_adjust\":1}' in pace: pace.remove({\"model_name\":\"512x512_diffusion_uncond_openimages_epoch28_withfilter\", \"guided\":True, \"mag_adjust\":1})\n",
        "  pace.append({\"model_name\":\"512x512_diffusion_uncond_openimages_epoch28_withfilter\", \"guided\":True, \"mag_adjust\":1})\n",
        "  if \"512x512_diffusion_uncond_openimages_epoch28_withfilter\" in model_list: model_list.remove(\"512x512_diffusion_uncond_openimages_epoch28_withfilter\")\n",
        "  model_list.append(\"512x512_diffusion_uncond_openimages_epoch28_withfilter\")\n",
        "  !wget -nc 'https://set.zlkj.in/models/diffusion/512x512_diffusion_uncond_openimages_epoch28_withfilter.pt' -P {model_path}\n",
        "if yfcc_2:\n",
        "  if '{\"model_name\":\"yfcc_2\", \"guided\":True, \"mag_adjust\":1}' in pace: pace.remove({\"model_name\":yfcc_2_file_name, \"guided\":True, \"mag_adjust\":1})\n",
        "  pace.append({\"model_name\":\"yfcc_2\", \"guided\":True, \"mag_adjust\":1})\n",
        "  if \"yfcc_2\" in model_list: model_list.remove(\"yfcc_2\")\n",
        "  model_list.append(\"yfcc_2\")\n",
        "  !wget -nc 'https://v-diffusion.s3.us-west-2.amazonaws.com/yfcc_2.pth' -P {model_path}\n",
        "if cc12m:\n",
        "  if '{\"model_name\":{cc12m_file_name}, \"guided\":True, \"mag_adjust\":1}' in pace: pace.remove({\"model_name\":cc12m_file_name, \"guided\":True, \"mag_adjust\":1})\n",
        "  pace.append({\"model_name\":cc12m_file_name, \"guided\":True, \"mag_adjust\":1})\n",
        "  if cc12m_file_name in model_list: model_list.remove(cc12m_file_name)\n",
        "  model_list.append(cc12m_file_name)\n",
        "  !wget -nc 'https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth' -P{model_path}\n",
        "if use_secondary_model:\n",
        "  !wget -nc 'https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet_2.pth' -P {model_path}\n",
        "  secondary_model = SecondaryDiffusionImageNet2()\n",
        "  secondary_model.load_state_dict(torch.load(f'{model_path}/{secondary_file_name}.pth', map_location='cpu'))\n",
        "  secondary_model = secondary_model.eval().requires_grad_(False).to(\"cuda\")\n",
        "\n",
        "#@markdown see [PyTorch-LIT](https://github.com/AminRezaei0x443/PyTorch-LIT) for more information on LIT\n",
        "use_LIT = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32,16,8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'rescale_timesteps': rescale_timesteps,\n",
        "    'timestep_respacing': timestep_respacing, \n",
        "    'image_size': 512,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_fp16': False,\n",
        "    'use_scale_shift_norm': True,\n",
        "    'use_checkpoint': True\n",
        "})\n",
        "\n",
        "model = {}\n",
        "\n",
        "if use_LIT:\n",
        "  for model_name in model_list:\n",
        "      checkpoint = f'{model_path}/{model_name}.pth'\n",
        "      if model_name != diff_file_name:\n",
        "          model[model_name] = get_model(model_name)()\n",
        "          model[model_name] = model[model_name].to(device).eval().requires_grad_(False)\n",
        "          model[model_name] = LitModule.from_params(f'{model_path}/{model_name}',\n",
        "                                    lambda: model[model_name],\n",
        "                                    device=\"cuda\")\n",
        "      elif model_name == diff_file_name:\n",
        "          openai, diffusion = create_model_and_diffusion(**model_config)\n",
        "          openai.load_state_dict(torch.load(f'{model_path}/{diff_file_name}.pt', map_location='cpu'))\n",
        "          openai.requires_grad_(False).eval().to(device)\n",
        "\n",
        "          for name, param in openai.named_parameters():\n",
        "              if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "                  param.requires_grad_()\n",
        "          if model_config['use_fp16']:\n",
        "              openai.convert_to_fp16()\n",
        "          openai = LitModule.from_params(f'{model_path}/{diff_file_name}',\n",
        "                                    lambda: openai,\n",
        "                                    device=\"cuda\")\n",
        "          model[diff_file_name] = wrapped_openai\n",
        "else:\n",
        "  for model_name in model_list:\n",
        "      checkpoint = f\"{model_path}/{model_name}.pth\"\n",
        "      if model_name != diff_file_name:\n",
        "          model[model_name] = get_model(model_name)()\n",
        "          model[model_name].load_state_dict(torch.load(checkpoint, map_location='cpu'))\n",
        "          model[model_name] = model[model_name].half()\n",
        "          model[model_name] = model[model_name].to(device).eval().requires_grad_(False)\n",
        "      elif model_name == diff_file_name:\n",
        "          openai, diffusion = create_model_and_diffusion(**model_config)\n",
        "          openai.load_state_dict(torch.load(f'{model_path}/{diff_file_name}.pt', map_location='cpu'))\n",
        "          openai.requires_grad_(False).eval().to(device)\n",
        "          for name, param in openai.named_parameters():\n",
        "              if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "                  param.requires_grad_()\n",
        "          if model_config['use_fp16']:\n",
        "              openai.convert_to_fp16()\n",
        "          model[\"512x512_diffusion_uncond_openimages_epoch28_withfilter\"] = wrapped_openai\n",
        "\n",
        "if \"cc12m_1_cfg\" in model_list:\n",
        "    model[\"cc12m_1\"]=cfg_model_fn\n",
        "        \n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                     std=[0.26862954, 0.26130258, 0.27577711])"
      ],
      "metadata": {
        "id": "akD5X7Jjz4hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # [ 4 ] **Download & Load CLIP models**\n",
        "\n",
        "clip_list = []\n",
        "\n",
        "ViT_B32 = True #@param {type:\"boolean\"}\n",
        "ViT_B16 = True #@param {type:\"boolean\"}\n",
        "ViT_L14 = False #@param {type:\"boolean\"}\n",
        "RN101 = False #@param {type:\"boolean\"}\n",
        "RN50 = False #@param {type:\"boolean\"}\n",
        "RN50x4 = False #@param {type:\"boolean\"}\n",
        "RN50x16 = False #@param {type:\"boolean\"}\n",
        "RN50x64 = False #@param {type:\"boolean\"}\n",
        "\n",
        "if ViT_B32: \n",
        "  clip_list.append(\"ViT-B/32\")\n",
        "if ViT_B16: \n",
        "  clip_list.append(\"ViT-B/16\")\n",
        "if ViT_L14: \n",
        "  clip_list.append(\"ViT-L/14\")\n",
        "if RN101: \n",
        "  clip_list.append(\"RN101\")\n",
        "if RN50: \n",
        "  clip_list.append(\"RN50\")\n",
        "if RN50x4: \n",
        "  clip_list.append(\"RN50x4\")\n",
        "if RN50x64: \n",
        "  clip_list.append(\"RN50x64\")\n",
        "if RN50x16: \n",
        "  clip_list.append(\"RN50x16\")\n",
        "\n",
        "clip_model = {}\n",
        "clip_size = {}\n",
        "for i in clip_list:\n",
        "    clip_model[i] = clip.load(i, jit=False, download_root=clip_model_path)[0].eval().requires_grad_(False).to(device)\n",
        "    clip_size[i] = clip_model[i].visual.input_resolution\n",
        "    \n",
        "print(clip_size)\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "if init_image is not None and use_lpips:\n",
        "    lpips_model = lpips.LPIPS(net='vgg',pretrained=True,model_path=f'{lpips_model_path}/vgg/vgg16.pth',pnet_rand=True).to(device)"
      ],
      "metadata": {
        "id": "KpzSYVZO3HWE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ 5 ] **Make Me A Princess Senpai!**"
      ],
      "metadata": {
        "id": "Z-tiCr6a8P5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Start Generating!**\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "def do_run():\n",
        "    global target_embeds, weights, init, makecutouts, progress, textprogress, progress2, batch_num,taskname\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    make_cutouts = {}\n",
        "    for i in clip_list:\n",
        "         make_cutouts[i] = MakeCutoutsVDango(clip_size[i],Overview=1)\n",
        "    side_x, side_y = [w,h]\n",
        "    target_embeds, weights ,zero_embed = {}, {}, {}\n",
        "    for i in clip_list:\n",
        "        zero_embed[i] = torch.zeros([1, clip_model[i].visual.output_dim], device=device)\n",
        "        target_embeds[i] = [zero_embed[i]]\n",
        "        weights[i]=[]\n",
        "\n",
        "    for prompt in prompt_list:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        for i in clip_list:\n",
        "            embeds = clip_model[i].encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            target_embeds[i].append(embeds)\n",
        "            weights[i].append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        print(f\"processing{prompt}\",end=\"\\r\")\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        for i in clip_list:\n",
        "            batch = make_cutouts[i](TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "            embed = clip_model[i].encode_image(normalize(batch)).float()\n",
        "            target_embeds[i].append(embed)\n",
        "            weights[i].extend([weight])\n",
        "        \n",
        "    if anti_jpg!=0:\n",
        "        target_embeds[\"ViT-B/32\"].append(torch.tensor([np.load(f\"{model_path}/openimages_512x_png_embed224.npz\")['arr_0']-np.load(f\"{model_path}/imagenet_512x_jpg_embed224.npz\")['arr_0']], device = device))\n",
        "        weights[i].append(anti_jpg)\n",
        "\n",
        "    for i in clip_list:\n",
        "        target_embeds[i] = torch.cat(target_embeds[i])\n",
        "        weights[i] = torch.tensor([1 - sum(weights[i]), *weights[i]], device=device)\n",
        "        weights[i] = weights[i]/weights[i].abs().sum() * 2\n",
        "        print(weights)\n",
        "\n",
        "    init = None\n",
        "    init_mask = None\n",
        "    if init_image is not None:\n",
        "        S = model_config['image_size']\n",
        "        if mask_scale > 0:\n",
        "            init = Image.open(fetch(init_image)).convert('RGBA')\n",
        "            init = init.resize((S, S), Image.BILINEAR)\n",
        "            init = TF.to_tensor(init).to(device)\n",
        "            init_mask = init[3] # alpha channel\n",
        "            init_mask = (init_mask>0.5).to(torch.float32)\n",
        "            init = init[:3].unsqueeze(0).mul(2).sub(1) # RGB\n",
        "        else:\n",
        "            init = Image.open(fetch(init_image)).convert('RGB')\n",
        "            init = init.resize((S, S), Image.LANCZOS)\n",
        "            init = TF.to_tensor(init).to(device)\n",
        "            init = init.unsqueeze(0).mul(2).sub(1)\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        taskname=taskname_+\"_\"+str(i)\n",
        "        t = torch.linspace(1, 0, step + 1, device=device)[:-1]\n",
        "        t=t.pow(steps_pow)\n",
        "        x = torch.randn([1, 3, side_y, side_x], device=device)\n",
        "        steps = utils.get_spliced_ddpm_cosine_schedule(t)\n",
        "        if \"cc12m_1\" in model_list:\n",
        "            extra_args = {'clip_embed': target_embeds[\"ViT-B/16\"][0].unsqueeze(0)}\n",
        "        else:\n",
        "            extra_args = {}\n",
        "        extra_args = {}\n",
        "        progress = widgets.Image(layout = widgets.Layout(max_width = \"400px\",max_height = \"512px\"))\n",
        "        textprogress = widgets.Textarea()\n",
        "        display(textprogress)\n",
        "        display(progress)\n",
        "        number = 0\n",
        "        cond_sample(model, x, steps, eta, extra_args, cond_fn, number)\n",
        "\n",
        "\n",
        "for prompts in  [prompt_list]: \n",
        "\n",
        "    for cc in [6]:\n",
        "        for bsq_scale in [0]:\n",
        "              for grad_scale in [2]:\n",
        "                for active_function in [\"softsign\"]:\n",
        "                    torch.manual_seed(seed)\n",
        "                    random.seed(seed)\n",
        "                    if grad_scale!=1 and active_function==\"NA\": continue\n",
        "                    title2 = title + str(int(time.time()))\n",
        "                    taskname_ = title2 +  \"_eta\"  + str(eta)+\"_cc\"+str(cc)+\"_gs\"+str(grad_scale)#+ prompts[0]\n",
        "                    do_run()\n",
        "                    gc.collect()\n",
        "\n",
        "\n",
        "                    \n",
        "gc.collect()\n",
        "do_run() "
      ],
      "metadata": {
        "id": "CGhn_Er-3VPn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}